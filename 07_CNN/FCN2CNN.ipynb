{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "FaadnhbpCcsh"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ly7lrx-gCuLy"
   },
   "source": [
    "# Xarxes convolucionals\n",
    "\n",
    "L'objectiu d'avui és la creació d'una xarxa convolucional que obtengui **com a mínim igual resultat que la xarxa completament connectada implementada la setmana anterior però amb menys paràmetres**. Per poder realitzar comparacions directes emprarem el mateix conjunt de dades.\n",
    "\n",
    "Com objectius secundaris tenim:\n",
    "\n",
    "1. Aprenentatge de noves estratègies per evitar `overfitting`.\n",
    "2. Us d'un nou optimitzador.\n",
    "3. Visualització dels resultats dels filtres convolucionals.\n",
    "\n",
    "Primer de tot, com sempre, les dades:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "PwSoPhjXCvV9"
   },
   "outputs": [],
   "source": [
    "# Recordau: El label del dataset és l'índex de la llista labels. Cada posició de la llista és un codi ASCII. Podeu emprar la funció chr per fer la transformació\n",
    "\n",
    "# Definim una seqüència (composició) de transformacions\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            (0.1307,), (0.3081,)\n",
    "        ),  # mitjana, desviacio tipica (precalculats)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Descarregam un dataset ja integrat en la llibreria Pytorch:\n",
    "train = datasets.EMNIST(\n",
    "    \"data\", split=\"digits\", train=True, download=True, transform=transform\n",
    ")  ## Si acabau podeu fer proves amb el split \"balanced\"\n",
    "test = datasets.EMNIST(\"data\", split=\"digits\", train=False, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_batch_size = 64\n",
    "test_batch_size = 100\n",
    "\n",
    "# Transformam les dades en l'estructura necessaria per entrenar una xarxa\n",
    "train_loader = torch.utils.data.DataLoader(train, train_batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test, test_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8i4Mg8KuD3r"
   },
   "source": [
    "## Definició de la xarxa\n",
    "\n",
    "### Feina a fer\n",
    "\n",
    "1. Definir la primera xarxa convolucional. A continuació teniu una llista de les capes que podeu emprar:\n",
    "\n",
    "-   `Conv2d`: Capa convolucional en 2 dimensions ([enllaç](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html)). Com a paràmetres principals trobarem:\n",
    "\n",
    "    -   in_channels: canals d'entrada.\n",
    "    -   out_channels : canals de sortida.\n",
    "    -   kernel_size: mida del filtre.\n",
    "    -   stride: desplaçament del filtre. Típicament pren per valor 1.\n",
    "    -   padding: ampliació de la imatge per evitar pèrdua de dimensionalitat.\n",
    "\n",
    "-   `MaxPool2d`: Capa de max pooling ([enllaç](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d)). Aquesta capa no té paràmetres entrenables. Però si:\n",
    "\n",
    "    -   kernel_size: Mida del filtre del qual es seleccionarà el màxim.\n",
    "    -   stride: desplaçament del filtre.\n",
    "\n",
    "-   `Dropout`: Dropout és un mètode de regularització (evitar `overfitting`) que aproxima l'entrenament d'un gran nombre de xarxes neuronals amb diferents arquitectures en paral·lel. Durant l'entrenament, una part de les sortides de la capa s'ignoren aleatòriament o s'abandonen. Això té l'efecte de fer que la capa sembli i es tracti com una capa amb un nombre diferent de nodes i connectivitat a la capa anterior. En efecte, cada actualització d'una capa durant l'entrenament es realitza amb una vista diferent de la capa configurada. Hem d'especificar quines capes tenen `dropout` de manera individual. Té un únic paràmetre amb valor per defecte $p=0.5$ Els valors típics d'aquest paràmetre varien entre $0.5$ i $0.8$.\n",
    "\n",
    "-   `Linear`\n",
    "\n",
    "-   `ReLU`\n",
    "\n",
    "2. Per posibilitar la visualització de les imatges passades per les capes convolucionals farem que funció `forward` tengui diverses sortides (diferents valors de `return`) un per cada capa convolucional de la xarxa.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IQvdRDtTHdRy"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(\n",
    "            in_channels=1, out_channels=8, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.conv_2 = nn.Conv2d(\n",
    "            in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.linear_1 = nn.Linear(7 * 7 * 16, 32)\n",
    "        self.linear_2 = nn.Linear(32, 10)\n",
    "        self.dropout = nn.Dropout()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        layer_order = (\n",
    "            (\"conv\", self.conv_1),\n",
    "            (\"pool\", self.max_pool),\n",
    "            (\"conv\", self.conv_2),\n",
    "            (\"pool\", self.max_pool),\n",
    "            (\"flat\", lambda x: torch.flatten(x, 1)),\n",
    "            (\"line\", self.linear_1),\n",
    "            (\"line\", self.linear_2),\n",
    "            (\"drop\", self.dropout),\n",
    "            (\"relu\", self.relu),\n",
    "            (\"sfmx\", lambda x: F.log_softmax(x, dim=1)),\n",
    "        )\n",
    "\n",
    "        data = x.clone()\n",
    "\n",
    "        data_outputs = []\n",
    "        for name, layer in layer_order:\n",
    "            data = layer(data)\n",
    "            data_outputs.append((name, data.clone()))\n",
    "\n",
    "        return data, data_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6ISOL_hCk7g"
   },
   "source": [
    "## Entrenament\n",
    "\n",
    "Aquesta part, per sort, no varia massa de la setmana anterior:\n",
    "\n",
    "### Feina a fer\n",
    "\n",
    "1. Modificar la sortida de la xarxa, ara retorna diversos valors, encara que aquí només us interessa un.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "h9OLtpPzClch"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    model, device, train_loader, optimizer, epoch, log_interval=100, verbose=True\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    loss_v = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, _ = model(data)  # això no us funcionarà\n",
    "        ## Aquesta setmana empram la cross_entropy com una funció\n",
    "        loss = F.cross_entropy(output, target, reduction=\"sum\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0 and verbose:\n",
    "            print(\n",
    "                \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}, Average: {:.6f}\".format(\n",
    "                    epoch,\n",
    "                    batch_idx * len(data),\n",
    "                    len(train_loader.dataset),\n",
    "                    100.0 * batch_idx / len(train_loader),\n",
    "                    loss.item(),\n",
    "                    loss.item() / len(data),\n",
    "                )\n",
    "            )\n",
    "        loss_v += loss.item()\n",
    "\n",
    "    loss_v /= len(train_loader.dataset)\n",
    "    print(\"\\nTrain set: Average loss: {:.4f}\\n\".format(loss_v))\n",
    "\n",
    "    return loss_v\n",
    "\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, _ = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, reduction=\"sum\")\n",
    "            pred = output.argmax(\n",
    "                dim=1, keepdim=True\n",
    "            )  # get the index of the max probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * correct / len(test_loader.dataset),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A continuació definim els paràmetres d'entrenament i el bucle principal:\n",
    "\n",
    "### Adam\n",
    "\n",
    "Aquesta setmana introduirem un nou algorisme d'optimització anomenat `Adam`. Fins ara hem emprat el descens del gradient (`SGD`).\n",
    "\n",
    "`Adam` és un algorisme d'optimització amplament emprat, tal com el descens del gradient, és iteratiu. A la literatura trobam arguments que indiquen que, tot i que Adam convergeix més ràpidament, SGD generalitza millor que Adam i, per tant, pot resultar en un rendiment final millor.\n",
    "\n",
    "[Més info](https://medium.com/geekculture/a-2021-guide-to-improving-cnns-optimizers-adam-vs-sgd-495848ac6008)\n",
    "\n",
    "### Feina a fer:\n",
    "\n",
    "1. Mostrar el nombre de paràmetres de la xarxa i compara-ho amb el nombre de paràmetres amb la xarxa de la setmana passada). Fes la teva xarxa més petita fins que el resultat decaigui.\n",
    "\n",
    "```\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "```\n",
    "\n",
    "2. Dibuixar els gràfics de la funció de pèrdua amb les dues funcions d'optimització que coneixem: comparar `SGD` amb `ADAM`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cNIBWqAwsVSb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/240000 (0%)]\tLoss: 146.837540, Average: 2.294337\n",
      "Train Epoch: 0 [6400/240000 (3%)]\tLoss: 145.073929, Average: 2.266780\n",
      "Train Epoch: 0 [12800/240000 (5%)]\tLoss: 144.125214, Average: 2.251956\n",
      "Train Epoch: 0 [19200/240000 (8%)]\tLoss: 145.489288, Average: 2.273270\n",
      "Train Epoch: 0 [25600/240000 (11%)]\tLoss: 141.731720, Average: 2.214558\n",
      "Train Epoch: 0 [32000/240000 (13%)]\tLoss: 143.571533, Average: 2.243305\n",
      "Train Epoch: 0 [38400/240000 (16%)]\tLoss: 137.490143, Average: 2.148283\n",
      "Train Epoch: 0 [44800/240000 (19%)]\tLoss: 137.147583, Average: 2.142931\n",
      "Train Epoch: 0 [51200/240000 (21%)]\tLoss: 125.747353, Average: 1.964802\n",
      "Train Epoch: 0 [57600/240000 (24%)]\tLoss: 130.292709, Average: 2.035824\n",
      "Train Epoch: 0 [64000/240000 (27%)]\tLoss: 128.073441, Average: 2.001148\n",
      "Train Epoch: 0 [70400/240000 (29%)]\tLoss: 128.211212, Average: 2.003300\n",
      "Train Epoch: 0 [76800/240000 (32%)]\tLoss: 114.848244, Average: 1.794504\n",
      "Train Epoch: 0 [83200/240000 (35%)]\tLoss: 115.304146, Average: 1.801627\n",
      "Train Epoch: 0 [89600/240000 (37%)]\tLoss: 119.242630, Average: 1.863166\n",
      "Train Epoch: 0 [96000/240000 (40%)]\tLoss: 133.389709, Average: 2.084214\n",
      "Train Epoch: 0 [102400/240000 (43%)]\tLoss: 105.999748, Average: 1.656246\n",
      "Train Epoch: 0 [108800/240000 (45%)]\tLoss: 124.785049, Average: 1.949766\n",
      "Train Epoch: 0 [115200/240000 (48%)]\tLoss: 124.069771, Average: 1.938590\n",
      "Train Epoch: 0 [121600/240000 (51%)]\tLoss: 123.788101, Average: 1.934189\n",
      "Train Epoch: 0 [128000/240000 (53%)]\tLoss: 110.830627, Average: 1.731729\n",
      "Train Epoch: 0 [134400/240000 (56%)]\tLoss: 108.697433, Average: 1.698397\n",
      "Train Epoch: 0 [140800/240000 (59%)]\tLoss: 105.311310, Average: 1.645489\n",
      "Train Epoch: 0 [147200/240000 (61%)]\tLoss: 121.465462, Average: 1.897898\n",
      "Train Epoch: 0 [153600/240000 (64%)]\tLoss: 101.791779, Average: 1.590497\n",
      "Train Epoch: 0 [160000/240000 (67%)]\tLoss: 103.377800, Average: 1.615278\n",
      "Train Epoch: 0 [166400/240000 (69%)]\tLoss: 106.589348, Average: 1.665459\n",
      "Train Epoch: 0 [172800/240000 (72%)]\tLoss: 112.320480, Average: 1.755008\n",
      "Train Epoch: 0 [179200/240000 (75%)]\tLoss: 94.760178, Average: 1.480628\n",
      "Train Epoch: 0 [185600/240000 (77%)]\tLoss: 90.210739, Average: 1.409543\n",
      "Train Epoch: 0 [192000/240000 (80%)]\tLoss: 95.322998, Average: 1.489422\n",
      "Train Epoch: 0 [198400/240000 (83%)]\tLoss: 106.930710, Average: 1.670792\n",
      "Train Epoch: 0 [204800/240000 (85%)]\tLoss: 107.926888, Average: 1.686358\n",
      "Train Epoch: 0 [211200/240000 (88%)]\tLoss: 113.942627, Average: 1.780354\n",
      "Train Epoch: 0 [217600/240000 (91%)]\tLoss: 90.140518, Average: 1.408446\n",
      "Train Epoch: 0 [224000/240000 (93%)]\tLoss: 107.637863, Average: 1.681842\n",
      "Train Epoch: 0 [230400/240000 (96%)]\tLoss: 104.266335, Average: 1.629161\n",
      "Train Epoch: 0 [236800/240000 (99%)]\tLoss: 100.298073, Average: 1.567157\n",
      "\n",
      "Train set: Average loss: 1.8464\n",
      "\n",
      "\n",
      "Test set: Average loss: 1.1843, Accuracy: 33614/40000 (84%)\n",
      "\n",
      "Train Epoch: 1 [0/240000 (0%)]\tLoss: 112.740326, Average: 1.761568\n",
      "Train Epoch: 1 [6400/240000 (3%)]\tLoss: 111.079285, Average: 1.735614\n",
      "Train Epoch: 1 [12800/240000 (5%)]\tLoss: 91.455559, Average: 1.428993\n",
      "Train Epoch: 1 [19200/240000 (8%)]\tLoss: 91.015121, Average: 1.422111\n",
      "Train Epoch: 1 [25600/240000 (11%)]\tLoss: 103.184059, Average: 1.612251\n",
      "Train Epoch: 1 [32000/240000 (13%)]\tLoss: 98.168823, Average: 1.533888\n",
      "Train Epoch: 1 [38400/240000 (16%)]\tLoss: 112.157036, Average: 1.752454\n",
      "Train Epoch: 1 [44800/240000 (19%)]\tLoss: 95.990540, Average: 1.499852\n",
      "Train Epoch: 1 [51200/240000 (21%)]\tLoss: 119.335800, Average: 1.864622\n",
      "Train Epoch: 1 [57600/240000 (24%)]\tLoss: 107.510941, Average: 1.679858\n",
      "Train Epoch: 1 [64000/240000 (27%)]\tLoss: 111.102448, Average: 1.735976\n",
      "Train Epoch: 1 [70400/240000 (29%)]\tLoss: 108.868179, Average: 1.701065\n",
      "Train Epoch: 1 [76800/240000 (32%)]\tLoss: 85.878212, Average: 1.341847\n",
      "Train Epoch: 1 [83200/240000 (35%)]\tLoss: 90.517609, Average: 1.414338\n",
      "Train Epoch: 1 [89600/240000 (37%)]\tLoss: 111.785980, Average: 1.746656\n",
      "Train Epoch: 1 [96000/240000 (40%)]\tLoss: 95.462646, Average: 1.491604\n",
      "Train Epoch: 1 [102400/240000 (43%)]\tLoss: 89.883820, Average: 1.404435\n",
      "Train Epoch: 1 [108800/240000 (45%)]\tLoss: 100.916626, Average: 1.576822\n",
      "Train Epoch: 1 [115200/240000 (48%)]\tLoss: 102.160439, Average: 1.596257\n",
      "Train Epoch: 1 [121600/240000 (51%)]\tLoss: 93.393227, Average: 1.459269\n",
      "Train Epoch: 1 [128000/240000 (53%)]\tLoss: 80.121864, Average: 1.251904\n",
      "Train Epoch: 1 [134400/240000 (56%)]\tLoss: 103.482117, Average: 1.616908\n",
      "Train Epoch: 1 [140800/240000 (59%)]\tLoss: 82.036301, Average: 1.281817\n",
      "Train Epoch: 1 [147200/240000 (61%)]\tLoss: 99.297417, Average: 1.551522\n",
      "Train Epoch: 1 [153600/240000 (64%)]\tLoss: 94.118614, Average: 1.470603\n",
      "Train Epoch: 1 [160000/240000 (67%)]\tLoss: 106.568527, Average: 1.665133\n",
      "Train Epoch: 1 [166400/240000 (69%)]\tLoss: 96.134880, Average: 1.502108\n",
      "Train Epoch: 1 [172800/240000 (72%)]\tLoss: 89.667496, Average: 1.401055\n",
      "Train Epoch: 1 [179200/240000 (75%)]\tLoss: 97.698242, Average: 1.526535\n",
      "Train Epoch: 1 [185600/240000 (77%)]\tLoss: 83.187630, Average: 1.299807\n",
      "Train Epoch: 1 [192000/240000 (80%)]\tLoss: 93.506699, Average: 1.461042\n",
      "Train Epoch: 1 [198400/240000 (83%)]\tLoss: 99.950203, Average: 1.561722\n",
      "Train Epoch: 1 [204800/240000 (85%)]\tLoss: 85.171883, Average: 1.330811\n",
      "Train Epoch: 1 [211200/240000 (88%)]\tLoss: 105.268250, Average: 1.644816\n",
      "Train Epoch: 1 [217600/240000 (91%)]\tLoss: 89.943626, Average: 1.405369\n",
      "Train Epoch: 1 [224000/240000 (93%)]\tLoss: 107.216110, Average: 1.675252\n",
      "Train Epoch: 1 [230400/240000 (96%)]\tLoss: 102.142990, Average: 1.595984\n",
      "Train Epoch: 1 [236800/240000 (99%)]\tLoss: 87.147034, Average: 1.361672\n",
      "\n",
      "Train set: Average loss: 1.5026\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.9231, Accuracy: 35872/40000 (90%)\n",
      "\n",
      "Train Epoch: 2 [0/240000 (0%)]\tLoss: 105.681732, Average: 1.651277\n",
      "Train Epoch: 2 [6400/240000 (3%)]\tLoss: 97.958992, Average: 1.530609\n",
      "Train Epoch: 2 [12800/240000 (5%)]\tLoss: 82.016891, Average: 1.281514\n",
      "Train Epoch: 2 [19200/240000 (8%)]\tLoss: 97.398247, Average: 1.521848\n",
      "Train Epoch: 2 [25600/240000 (11%)]\tLoss: 100.700333, Average: 1.573443\n",
      "Train Epoch: 2 [32000/240000 (13%)]\tLoss: 84.742905, Average: 1.324108\n",
      "Train Epoch: 2 [38400/240000 (16%)]\tLoss: 102.686394, Average: 1.604475\n",
      "Train Epoch: 2 [44800/240000 (19%)]\tLoss: 110.650963, Average: 1.728921\n",
      "Train Epoch: 2 [51200/240000 (21%)]\tLoss: 109.552887, Average: 1.711764\n",
      "Train Epoch: 2 [57600/240000 (24%)]\tLoss: 77.530449, Average: 1.211413\n",
      "Train Epoch: 2 [64000/240000 (27%)]\tLoss: 92.706093, Average: 1.448533\n",
      "Train Epoch: 2 [70400/240000 (29%)]\tLoss: 100.454124, Average: 1.569596\n",
      "Train Epoch: 2 [76800/240000 (32%)]\tLoss: 100.376610, Average: 1.568385\n",
      "Train Epoch: 2 [83200/240000 (35%)]\tLoss: 79.417450, Average: 1.240898\n",
      "Train Epoch: 2 [89600/240000 (37%)]\tLoss: 118.390518, Average: 1.849852\n",
      "Train Epoch: 2 [96000/240000 (40%)]\tLoss: 100.436722, Average: 1.569324\n",
      "Train Epoch: 2 [102400/240000 (43%)]\tLoss: 82.672218, Average: 1.291753\n",
      "Train Epoch: 2 [108800/240000 (45%)]\tLoss: 87.006416, Average: 1.359475\n",
      "Train Epoch: 2 [115200/240000 (48%)]\tLoss: 97.680367, Average: 1.526256\n",
      "Train Epoch: 2 [121600/240000 (51%)]\tLoss: 87.810448, Average: 1.372038\n",
      "Train Epoch: 2 [128000/240000 (53%)]\tLoss: 85.227043, Average: 1.331673\n",
      "Train Epoch: 2 [134400/240000 (56%)]\tLoss: 79.687607, Average: 1.245119\n",
      "Train Epoch: 2 [140800/240000 (59%)]\tLoss: 84.020874, Average: 1.312826\n",
      "Train Epoch: 2 [147200/240000 (61%)]\tLoss: 86.050385, Average: 1.344537\n",
      "Train Epoch: 2 [153600/240000 (64%)]\tLoss: 73.335548, Average: 1.145868\n",
      "Train Epoch: 2 [160000/240000 (67%)]\tLoss: 96.927116, Average: 1.514486\n",
      "Train Epoch: 2 [166400/240000 (69%)]\tLoss: 92.224297, Average: 1.441005\n",
      "Train Epoch: 2 [172800/240000 (72%)]\tLoss: 93.637085, Average: 1.463079\n",
      "Train Epoch: 2 [179200/240000 (75%)]\tLoss: 106.506775, Average: 1.664168\n",
      "Train Epoch: 2 [185600/240000 (77%)]\tLoss: 92.166412, Average: 1.440100\n",
      "Train Epoch: 2 [192000/240000 (80%)]\tLoss: 89.189056, Average: 1.393579\n",
      "Train Epoch: 2 [198400/240000 (83%)]\tLoss: 86.442299, Average: 1.350661\n",
      "Train Epoch: 2 [204800/240000 (85%)]\tLoss: 98.328629, Average: 1.536385\n",
      "Train Epoch: 2 [211200/240000 (88%)]\tLoss: 100.582840, Average: 1.571607\n",
      "Train Epoch: 2 [217600/240000 (91%)]\tLoss: 104.872116, Average: 1.638627\n",
      "Train Epoch: 2 [224000/240000 (93%)]\tLoss: 80.183357, Average: 1.252865\n",
      "Train Epoch: 2 [230400/240000 (96%)]\tLoss: 88.319366, Average: 1.379990\n",
      "Train Epoch: 2 [236800/240000 (99%)]\tLoss: 109.838791, Average: 1.716231\n",
      "\n",
      "Train set: Average loss: 1.4540\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.8302, Accuracy: 36576/40000 (91%)\n",
      "\n",
      "Train Epoch: 3 [0/240000 (0%)]\tLoss: 109.539421, Average: 1.711553\n",
      "Train Epoch: 3 [6400/240000 (3%)]\tLoss: 92.568947, Average: 1.446390\n",
      "Train Epoch: 3 [12800/240000 (5%)]\tLoss: 79.484566, Average: 1.241946\n",
      "Train Epoch: 3 [19200/240000 (8%)]\tLoss: 77.505714, Average: 1.211027\n",
      "Train Epoch: 3 [25600/240000 (11%)]\tLoss: 92.407349, Average: 1.443865\n",
      "Train Epoch: 3 [32000/240000 (13%)]\tLoss: 90.591156, Average: 1.415487\n",
      "Train Epoch: 3 [38400/240000 (16%)]\tLoss: 103.433174, Average: 1.616143\n",
      "Train Epoch: 3 [44800/240000 (19%)]\tLoss: 85.030571, Average: 1.328603\n",
      "Train Epoch: 3 [51200/240000 (21%)]\tLoss: 91.891014, Average: 1.435797\n",
      "Train Epoch: 3 [57600/240000 (24%)]\tLoss: 111.942108, Average: 1.749095\n",
      "Train Epoch: 3 [64000/240000 (27%)]\tLoss: 90.640259, Average: 1.416254\n",
      "Train Epoch: 3 [70400/240000 (29%)]\tLoss: 97.428925, Average: 1.522327\n",
      "Train Epoch: 3 [76800/240000 (32%)]\tLoss: 81.796173, Average: 1.278065\n",
      "Train Epoch: 3 [83200/240000 (35%)]\tLoss: 100.256187, Average: 1.566503\n",
      "Train Epoch: 3 [89600/240000 (37%)]\tLoss: 94.114159, Average: 1.470534\n",
      "Train Epoch: 3 [96000/240000 (40%)]\tLoss: 73.339867, Average: 1.145935\n",
      "Train Epoch: 3 [102400/240000 (43%)]\tLoss: 91.190468, Average: 1.424851\n",
      "Train Epoch: 3 [108800/240000 (45%)]\tLoss: 94.105026, Average: 1.470391\n",
      "Train Epoch: 3 [115200/240000 (48%)]\tLoss: 94.131134, Average: 1.470799\n",
      "Train Epoch: 3 [121600/240000 (51%)]\tLoss: 81.936546, Average: 1.280259\n",
      "Train Epoch: 3 [128000/240000 (53%)]\tLoss: 79.169189, Average: 1.237019\n",
      "Train Epoch: 3 [134400/240000 (56%)]\tLoss: 85.101677, Average: 1.329714\n",
      "Train Epoch: 3 [140800/240000 (59%)]\tLoss: 89.236343, Average: 1.394318\n",
      "Train Epoch: 3 [147200/240000 (61%)]\tLoss: 86.658272, Average: 1.354035\n",
      "Train Epoch: 3 [153600/240000 (64%)]\tLoss: 80.891670, Average: 1.263932\n",
      "Train Epoch: 3 [160000/240000 (67%)]\tLoss: 101.808228, Average: 1.590754\n",
      "Train Epoch: 3 [166400/240000 (69%)]\tLoss: 94.356339, Average: 1.474318\n",
      "Train Epoch: 3 [172800/240000 (72%)]\tLoss: 83.420570, Average: 1.303446\n",
      "Train Epoch: 3 [179200/240000 (75%)]\tLoss: 83.998985, Average: 1.312484\n",
      "Train Epoch: 3 [185600/240000 (77%)]\tLoss: 104.157173, Average: 1.627456\n",
      "Train Epoch: 3 [192000/240000 (80%)]\tLoss: 103.333160, Average: 1.614581\n",
      "Train Epoch: 3 [198400/240000 (83%)]\tLoss: 75.436836, Average: 1.178701\n",
      "Train Epoch: 3 [204800/240000 (85%)]\tLoss: 93.475983, Average: 1.460562\n",
      "Train Epoch: 3 [211200/240000 (88%)]\tLoss: 74.839134, Average: 1.169361\n",
      "Train Epoch: 3 [217600/240000 (91%)]\tLoss: 83.427643, Average: 1.303557\n",
      "Train Epoch: 3 [224000/240000 (93%)]\tLoss: 85.597809, Average: 1.337466\n",
      "Train Epoch: 3 [230400/240000 (96%)]\tLoss: 92.834953, Average: 1.450546\n",
      "Train Epoch: 3 [236800/240000 (99%)]\tLoss: 101.005615, Average: 1.578213\n",
      "\n",
      "Train set: Average loss: 1.4298\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.7721, Accuracy: 36926/40000 (92%)\n",
      "\n",
      "Train Epoch: 4 [0/240000 (0%)]\tLoss: 107.817345, Average: 1.684646\n",
      "Train Epoch: 4 [6400/240000 (3%)]\tLoss: 109.737915, Average: 1.714655\n",
      "Train Epoch: 4 [12800/240000 (5%)]\tLoss: 80.941025, Average: 1.264704\n",
      "Train Epoch: 4 [19200/240000 (8%)]\tLoss: 95.210854, Average: 1.487670\n",
      "Train Epoch: 4 [25600/240000 (11%)]\tLoss: 79.202820, Average: 1.237544\n",
      "Train Epoch: 4 [32000/240000 (13%)]\tLoss: 91.136963, Average: 1.424015\n",
      "Train Epoch: 4 [38400/240000 (16%)]\tLoss: 95.464752, Average: 1.491637\n",
      "Train Epoch: 4 [44800/240000 (19%)]\tLoss: 85.968704, Average: 1.343261\n",
      "Train Epoch: 4 [51200/240000 (21%)]\tLoss: 106.237198, Average: 1.659956\n",
      "Train Epoch: 4 [57600/240000 (24%)]\tLoss: 96.505791, Average: 1.507903\n",
      "Train Epoch: 4 [64000/240000 (27%)]\tLoss: 86.596603, Average: 1.353072\n",
      "Train Epoch: 4 [70400/240000 (29%)]\tLoss: 77.673470, Average: 1.213648\n",
      "Train Epoch: 4 [76800/240000 (32%)]\tLoss: 65.494614, Average: 1.023353\n",
      "Train Epoch: 4 [83200/240000 (35%)]\tLoss: 101.954231, Average: 1.593035\n",
      "Train Epoch: 4 [89600/240000 (37%)]\tLoss: 90.390533, Average: 1.412352\n",
      "Train Epoch: 4 [96000/240000 (40%)]\tLoss: 96.530502, Average: 1.508289\n",
      "Train Epoch: 4 [102400/240000 (43%)]\tLoss: 76.198624, Average: 1.190603\n",
      "Train Epoch: 4 [108800/240000 (45%)]\tLoss: 80.701607, Average: 1.260963\n",
      "Train Epoch: 4 [115200/240000 (48%)]\tLoss: 94.829002, Average: 1.481703\n",
      "Train Epoch: 4 [121600/240000 (51%)]\tLoss: 82.044212, Average: 1.281941\n",
      "Train Epoch: 4 [128000/240000 (53%)]\tLoss: 89.099098, Average: 1.392173\n",
      "Train Epoch: 4 [134400/240000 (56%)]\tLoss: 95.099060, Average: 1.485923\n",
      "Train Epoch: 4 [140800/240000 (59%)]\tLoss: 82.687256, Average: 1.291988\n",
      "Train Epoch: 4 [147200/240000 (61%)]\tLoss: 87.696548, Average: 1.370259\n",
      "Train Epoch: 4 [153600/240000 (64%)]\tLoss: 93.283806, Average: 1.457559\n",
      "Train Epoch: 4 [160000/240000 (67%)]\tLoss: 88.210350, Average: 1.378287\n",
      "Train Epoch: 4 [166400/240000 (69%)]\tLoss: 96.367378, Average: 1.505740\n",
      "Train Epoch: 4 [172800/240000 (72%)]\tLoss: 95.903023, Average: 1.498485\n",
      "Train Epoch: 4 [179200/240000 (75%)]\tLoss: 86.033546, Average: 1.344274\n",
      "Train Epoch: 4 [185600/240000 (77%)]\tLoss: 93.623535, Average: 1.462868\n",
      "Train Epoch: 4 [192000/240000 (80%)]\tLoss: 88.439453, Average: 1.381866\n",
      "Train Epoch: 4 [198400/240000 (83%)]\tLoss: 90.511772, Average: 1.414246\n",
      "Train Epoch: 4 [204800/240000 (85%)]\tLoss: 74.795387, Average: 1.168678\n",
      "Train Epoch: 4 [211200/240000 (88%)]\tLoss: 109.211456, Average: 1.706429\n",
      "Train Epoch: 4 [217600/240000 (91%)]\tLoss: 79.240211, Average: 1.238128\n",
      "Train Epoch: 4 [224000/240000 (93%)]\tLoss: 87.344093, Average: 1.364751\n",
      "Train Epoch: 4 [230400/240000 (96%)]\tLoss: 93.037651, Average: 1.453713\n",
      "Train Epoch: 4 [236800/240000 (99%)]\tLoss: 97.430832, Average: 1.522357\n",
      "\n",
      "Train set: Average loss: 1.4193\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.7349, Accuracy: 37291/40000 (93%)\n",
      "\n",
      "Train Epoch: 5 [0/240000 (0%)]\tLoss: 98.491982, Average: 1.538937\n",
      "Train Epoch: 5 [6400/240000 (3%)]\tLoss: 93.748444, Average: 1.464819\n",
      "Train Epoch: 5 [12800/240000 (5%)]\tLoss: 92.680519, Average: 1.448133\n",
      "Train Epoch: 5 [19200/240000 (8%)]\tLoss: 83.467445, Average: 1.304179\n",
      "Train Epoch: 5 [25600/240000 (11%)]\tLoss: 106.847603, Average: 1.669494\n",
      "Train Epoch: 5 [32000/240000 (13%)]\tLoss: 94.336205, Average: 1.474003\n",
      "Train Epoch: 5 [38400/240000 (16%)]\tLoss: 80.535339, Average: 1.258365\n",
      "Train Epoch: 5 [44800/240000 (19%)]\tLoss: 83.425453, Average: 1.303523\n",
      "Train Epoch: 5 [51200/240000 (21%)]\tLoss: 89.809158, Average: 1.403268\n",
      "Train Epoch: 5 [57600/240000 (24%)]\tLoss: 103.448029, Average: 1.616375\n",
      "Train Epoch: 5 [64000/240000 (27%)]\tLoss: 101.177719, Average: 1.580902\n",
      "Train Epoch: 5 [70400/240000 (29%)]\tLoss: 98.579391, Average: 1.540303\n",
      "Train Epoch: 5 [76800/240000 (32%)]\tLoss: 92.987198, Average: 1.452925\n",
      "Train Epoch: 5 [83200/240000 (35%)]\tLoss: 87.134041, Average: 1.361469\n",
      "Train Epoch: 5 [89600/240000 (37%)]\tLoss: 87.527107, Average: 1.367611\n",
      "Train Epoch: 5 [96000/240000 (40%)]\tLoss: 75.241234, Average: 1.175644\n",
      "Train Epoch: 5 [102400/240000 (43%)]\tLoss: 92.891220, Average: 1.451425\n",
      "Train Epoch: 5 [108800/240000 (45%)]\tLoss: 92.268669, Average: 1.441698\n",
      "Train Epoch: 5 [115200/240000 (48%)]\tLoss: 101.291763, Average: 1.582684\n",
      "Train Epoch: 5 [121600/240000 (51%)]\tLoss: 79.663269, Average: 1.244739\n",
      "Train Epoch: 5 [128000/240000 (53%)]\tLoss: 79.730492, Average: 1.245789\n",
      "Train Epoch: 5 [134400/240000 (56%)]\tLoss: 77.716743, Average: 1.214324\n",
      "Train Epoch: 5 [140800/240000 (59%)]\tLoss: 78.456635, Average: 1.225885\n",
      "Train Epoch: 5 [147200/240000 (61%)]\tLoss: 80.469620, Average: 1.257338\n",
      "Train Epoch: 5 [153600/240000 (64%)]\tLoss: 91.071602, Average: 1.422994\n",
      "Train Epoch: 5 [160000/240000 (67%)]\tLoss: 108.093102, Average: 1.688955\n",
      "Train Epoch: 5 [166400/240000 (69%)]\tLoss: 90.742393, Average: 1.417850\n",
      "Train Epoch: 5 [172800/240000 (72%)]\tLoss: 102.884415, Average: 1.607569\n",
      "Train Epoch: 5 [179200/240000 (75%)]\tLoss: 84.732475, Average: 1.323945\n",
      "Train Epoch: 5 [185600/240000 (77%)]\tLoss: 80.353844, Average: 1.255529\n",
      "Train Epoch: 5 [192000/240000 (80%)]\tLoss: 88.454460, Average: 1.382101\n",
      "Train Epoch: 5 [198400/240000 (83%)]\tLoss: 107.081009, Average: 1.673141\n",
      "Train Epoch: 5 [204800/240000 (85%)]\tLoss: 85.654991, Average: 1.338359\n",
      "Train Epoch: 5 [211200/240000 (88%)]\tLoss: 105.458542, Average: 1.647790\n",
      "Train Epoch: 5 [217600/240000 (91%)]\tLoss: 102.731598, Average: 1.605181\n",
      "Train Epoch: 5 [224000/240000 (93%)]\tLoss: 106.720383, Average: 1.667506\n",
      "Train Epoch: 5 [230400/240000 (96%)]\tLoss: 87.810654, Average: 1.372041\n",
      "Train Epoch: 5 [236800/240000 (99%)]\tLoss: 91.970879, Average: 1.437045\n",
      "\n",
      "Train set: Average loss: 1.4052\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.7115, Accuracy: 37490/40000 (94%)\n",
      "\n",
      "Train Epoch: 6 [0/240000 (0%)]\tLoss: 89.232712, Average: 1.394261\n",
      "Train Epoch: 6 [6400/240000 (3%)]\tLoss: 83.511414, Average: 1.304866\n",
      "Train Epoch: 6 [12800/240000 (5%)]\tLoss: 85.687874, Average: 1.338873\n",
      "Train Epoch: 6 [19200/240000 (8%)]\tLoss: 68.963043, Average: 1.077548\n",
      "Train Epoch: 6 [25600/240000 (11%)]\tLoss: 93.382805, Average: 1.459106\n",
      "Train Epoch: 6 [32000/240000 (13%)]\tLoss: 91.542992, Average: 1.430359\n",
      "Train Epoch: 6 [38400/240000 (16%)]\tLoss: 90.953674, Average: 1.421151\n",
      "Train Epoch: 6 [44800/240000 (19%)]\tLoss: 85.075455, Average: 1.329304\n",
      "Train Epoch: 6 [51200/240000 (21%)]\tLoss: 107.886642, Average: 1.685729\n",
      "Train Epoch: 6 [57600/240000 (24%)]\tLoss: 94.721764, Average: 1.480028\n",
      "Train Epoch: 6 [64000/240000 (27%)]\tLoss: 91.655334, Average: 1.432115\n",
      "Train Epoch: 6 [70400/240000 (29%)]\tLoss: 76.998680, Average: 1.203104\n",
      "Train Epoch: 6 [76800/240000 (32%)]\tLoss: 108.050232, Average: 1.688285\n",
      "Train Epoch: 6 [83200/240000 (35%)]\tLoss: 86.766548, Average: 1.355727\n",
      "Train Epoch: 6 [89600/240000 (37%)]\tLoss: 104.463531, Average: 1.632243\n",
      "Train Epoch: 6 [96000/240000 (40%)]\tLoss: 97.426285, Average: 1.522286\n",
      "Train Epoch: 6 [102400/240000 (43%)]\tLoss: 95.111794, Average: 1.486122\n",
      "Train Epoch: 6 [108800/240000 (45%)]\tLoss: 96.548439, Average: 1.508569\n",
      "Train Epoch: 6 [115200/240000 (48%)]\tLoss: 99.277000, Average: 1.551203\n",
      "Train Epoch: 6 [121600/240000 (51%)]\tLoss: 98.269653, Average: 1.535463\n",
      "Train Epoch: 6 [128000/240000 (53%)]\tLoss: 102.875122, Average: 1.607424\n",
      "Train Epoch: 6 [134400/240000 (56%)]\tLoss: 90.126099, Average: 1.408220\n",
      "Train Epoch: 6 [140800/240000 (59%)]\tLoss: 86.231766, Average: 1.347371\n",
      "Train Epoch: 6 [147200/240000 (61%)]\tLoss: 93.829292, Average: 1.466083\n",
      "Train Epoch: 6 [153600/240000 (64%)]\tLoss: 81.104950, Average: 1.267265\n",
      "Train Epoch: 6 [160000/240000 (67%)]\tLoss: 106.265114, Average: 1.660392\n",
      "Train Epoch: 6 [166400/240000 (69%)]\tLoss: 93.313316, Average: 1.458021\n",
      "Train Epoch: 6 [172800/240000 (72%)]\tLoss: 91.196571, Average: 1.424946\n",
      "Train Epoch: 6 [179200/240000 (75%)]\tLoss: 84.787666, Average: 1.324807\n",
      "Train Epoch: 6 [185600/240000 (77%)]\tLoss: 85.490425, Average: 1.335788\n",
      "Train Epoch: 6 [192000/240000 (80%)]\tLoss: 76.215416, Average: 1.190866\n",
      "Train Epoch: 6 [198400/240000 (83%)]\tLoss: 67.692947, Average: 1.057702\n",
      "Train Epoch: 6 [204800/240000 (85%)]\tLoss: 85.088577, Average: 1.329509\n",
      "Train Epoch: 6 [211200/240000 (88%)]\tLoss: 79.233246, Average: 1.238019\n",
      "Train Epoch: 6 [217600/240000 (91%)]\tLoss: 68.739883, Average: 1.074061\n",
      "Train Epoch: 6 [224000/240000 (93%)]\tLoss: 95.826759, Average: 1.497293\n",
      "Train Epoch: 6 [230400/240000 (96%)]\tLoss: 91.413391, Average: 1.428334\n",
      "Train Epoch: 6 [236800/240000 (99%)]\tLoss: 97.977051, Average: 1.530891\n",
      "\n",
      "Train set: Average loss: 1.4000\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6841, Accuracy: 37645/40000 (94%)\n",
      "\n",
      "Train Epoch: 7 [0/240000 (0%)]\tLoss: 91.053116, Average: 1.422705\n",
      "Train Epoch: 7 [6400/240000 (3%)]\tLoss: 82.290855, Average: 1.285795\n",
      "Train Epoch: 7 [12800/240000 (5%)]\tLoss: 97.143387, Average: 1.517865\n",
      "Train Epoch: 7 [19200/240000 (8%)]\tLoss: 69.371391, Average: 1.083928\n",
      "Train Epoch: 7 [25600/240000 (11%)]\tLoss: 89.107285, Average: 1.392301\n",
      "Train Epoch: 7 [32000/240000 (13%)]\tLoss: 82.812958, Average: 1.293952\n",
      "Train Epoch: 7 [38400/240000 (16%)]\tLoss: 87.991348, Average: 1.374865\n",
      "Train Epoch: 7 [44800/240000 (19%)]\tLoss: 91.138275, Average: 1.424036\n",
      "Train Epoch: 7 [51200/240000 (21%)]\tLoss: 112.822006, Average: 1.762844\n",
      "Train Epoch: 7 [57600/240000 (24%)]\tLoss: 91.391769, Average: 1.427996\n",
      "Train Epoch: 7 [64000/240000 (27%)]\tLoss: 84.305130, Average: 1.317268\n",
      "Train Epoch: 7 [70400/240000 (29%)]\tLoss: 81.051498, Average: 1.266430\n",
      "Train Epoch: 7 [76800/240000 (32%)]\tLoss: 87.877861, Average: 1.373092\n",
      "Train Epoch: 7 [83200/240000 (35%)]\tLoss: 82.576492, Average: 1.290258\n",
      "Train Epoch: 7 [89600/240000 (37%)]\tLoss: 74.835861, Average: 1.169310\n",
      "Train Epoch: 7 [96000/240000 (40%)]\tLoss: 88.775421, Average: 1.387116\n",
      "Train Epoch: 7 [102400/240000 (43%)]\tLoss: 72.942451, Average: 1.139726\n",
      "Train Epoch: 7 [108800/240000 (45%)]\tLoss: 97.060440, Average: 1.516569\n",
      "Train Epoch: 7 [115200/240000 (48%)]\tLoss: 112.912941, Average: 1.764265\n",
      "Train Epoch: 7 [121600/240000 (51%)]\tLoss: 74.709908, Average: 1.167342\n",
      "Train Epoch: 7 [128000/240000 (53%)]\tLoss: 96.058861, Average: 1.500920\n",
      "Train Epoch: 7 [134400/240000 (56%)]\tLoss: 84.939140, Average: 1.327174\n",
      "Train Epoch: 7 [140800/240000 (59%)]\tLoss: 98.387245, Average: 1.537301\n",
      "Train Epoch: 7 [147200/240000 (61%)]\tLoss: 92.797768, Average: 1.449965\n",
      "Train Epoch: 7 [153600/240000 (64%)]\tLoss: 93.021027, Average: 1.453454\n",
      "Train Epoch: 7 [160000/240000 (67%)]\tLoss: 97.302399, Average: 1.520350\n",
      "Train Epoch: 7 [166400/240000 (69%)]\tLoss: 107.951721, Average: 1.686746\n",
      "Train Epoch: 7 [172800/240000 (72%)]\tLoss: 81.450256, Average: 1.272660\n",
      "Train Epoch: 7 [179200/240000 (75%)]\tLoss: 96.381065, Average: 1.505954\n",
      "Train Epoch: 7 [185600/240000 (77%)]\tLoss: 92.197388, Average: 1.440584\n",
      "Train Epoch: 7 [192000/240000 (80%)]\tLoss: 102.666618, Average: 1.604166\n",
      "Train Epoch: 7 [198400/240000 (83%)]\tLoss: 89.408882, Average: 1.397014\n",
      "Train Epoch: 7 [204800/240000 (85%)]\tLoss: 81.829918, Average: 1.278592\n",
      "Train Epoch: 7 [211200/240000 (88%)]\tLoss: 92.406982, Average: 1.443859\n",
      "Train Epoch: 7 [217600/240000 (91%)]\tLoss: 72.313950, Average: 1.129905\n",
      "Train Epoch: 7 [224000/240000 (93%)]\tLoss: 86.446289, Average: 1.350723\n",
      "Train Epoch: 7 [230400/240000 (96%)]\tLoss: 83.905319, Average: 1.311021\n",
      "Train Epoch: 7 [236800/240000 (99%)]\tLoss: 110.919731, Average: 1.733121\n",
      "\n",
      "Train set: Average loss: 1.3871\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6628, Accuracy: 37851/40000 (95%)\n",
      "\n",
      "Train Epoch: 8 [0/240000 (0%)]\tLoss: 86.572350, Average: 1.352693\n",
      "Train Epoch: 8 [6400/240000 (3%)]\tLoss: 85.669693, Average: 1.338589\n",
      "Train Epoch: 8 [12800/240000 (5%)]\tLoss: 78.392929, Average: 1.224890\n",
      "Train Epoch: 8 [19200/240000 (8%)]\tLoss: 78.762428, Average: 1.230663\n",
      "Train Epoch: 8 [25600/240000 (11%)]\tLoss: 83.181793, Average: 1.299716\n",
      "Train Epoch: 8 [32000/240000 (13%)]\tLoss: 98.816833, Average: 1.544013\n",
      "Train Epoch: 8 [38400/240000 (16%)]\tLoss: 87.804893, Average: 1.371951\n",
      "Train Epoch: 8 [44800/240000 (19%)]\tLoss: 98.369415, Average: 1.537022\n",
      "Train Epoch: 8 [51200/240000 (21%)]\tLoss: 110.706360, Average: 1.729787\n",
      "Train Epoch: 8 [57600/240000 (24%)]\tLoss: 99.229706, Average: 1.550464\n",
      "Train Epoch: 8 [64000/240000 (27%)]\tLoss: 89.514931, Average: 1.398671\n",
      "Train Epoch: 8 [70400/240000 (29%)]\tLoss: 90.853401, Average: 1.419584\n",
      "Train Epoch: 8 [76800/240000 (32%)]\tLoss: 72.108536, Average: 1.126696\n",
      "Train Epoch: 8 [83200/240000 (35%)]\tLoss: 84.509277, Average: 1.320457\n",
      "Train Epoch: 8 [89600/240000 (37%)]\tLoss: 85.146828, Average: 1.330419\n",
      "Train Epoch: 8 [96000/240000 (40%)]\tLoss: 91.050568, Average: 1.422665\n",
      "Train Epoch: 8 [102400/240000 (43%)]\tLoss: 103.633324, Average: 1.619271\n",
      "Train Epoch: 8 [108800/240000 (45%)]\tLoss: 76.311607, Average: 1.192369\n",
      "Train Epoch: 8 [115200/240000 (48%)]\tLoss: 96.182739, Average: 1.502855\n",
      "Train Epoch: 8 [121600/240000 (51%)]\tLoss: 98.108253, Average: 1.532941\n",
      "Train Epoch: 8 [128000/240000 (53%)]\tLoss: 108.096519, Average: 1.689008\n",
      "Train Epoch: 8 [134400/240000 (56%)]\tLoss: 85.000641, Average: 1.328135\n",
      "Train Epoch: 8 [140800/240000 (59%)]\tLoss: 74.944435, Average: 1.171007\n",
      "Train Epoch: 8 [147200/240000 (61%)]\tLoss: 85.887283, Average: 1.341989\n",
      "Train Epoch: 8 [153600/240000 (64%)]\tLoss: 95.471718, Average: 1.491746\n",
      "Train Epoch: 8 [160000/240000 (67%)]\tLoss: 88.791710, Average: 1.387370\n",
      "Train Epoch: 8 [166400/240000 (69%)]\tLoss: 92.929482, Average: 1.452023\n",
      "Train Epoch: 8 [172800/240000 (72%)]\tLoss: 84.605545, Average: 1.321962\n",
      "Train Epoch: 8 [179200/240000 (75%)]\tLoss: 103.641258, Average: 1.619395\n",
      "Train Epoch: 8 [185600/240000 (77%)]\tLoss: 77.432983, Average: 1.209890\n",
      "Train Epoch: 8 [192000/240000 (80%)]\tLoss: 82.994644, Average: 1.296791\n",
      "Train Epoch: 8 [198400/240000 (83%)]\tLoss: 76.610123, Average: 1.197033\n",
      "Train Epoch: 8 [204800/240000 (85%)]\tLoss: 93.896919, Average: 1.467139\n",
      "Train Epoch: 8 [211200/240000 (88%)]\tLoss: 88.958702, Average: 1.389980\n",
      "Train Epoch: 8 [217600/240000 (91%)]\tLoss: 83.318222, Average: 1.301847\n",
      "Train Epoch: 8 [224000/240000 (93%)]\tLoss: 77.236633, Average: 1.206822\n",
      "Train Epoch: 8 [230400/240000 (96%)]\tLoss: 86.788048, Average: 1.356063\n",
      "Train Epoch: 8 [236800/240000 (99%)]\tLoss: 80.484787, Average: 1.257575\n",
      "\n",
      "Train set: Average loss: 1.3795\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6447, Accuracy: 37970/40000 (95%)\n",
      "\n",
      "Train Epoch: 9 [0/240000 (0%)]\tLoss: 97.738510, Average: 1.527164\n",
      "Train Epoch: 9 [6400/240000 (3%)]\tLoss: 91.767807, Average: 1.433872\n",
      "Train Epoch: 9 [12800/240000 (5%)]\tLoss: 90.708778, Average: 1.417325\n",
      "Train Epoch: 9 [19200/240000 (8%)]\tLoss: 79.498909, Average: 1.242170\n",
      "Train Epoch: 9 [25600/240000 (11%)]\tLoss: 70.559555, Average: 1.102493\n",
      "Train Epoch: 9 [32000/240000 (13%)]\tLoss: 75.809715, Average: 1.184527\n",
      "Train Epoch: 9 [38400/240000 (16%)]\tLoss: 93.508514, Average: 1.461071\n",
      "Train Epoch: 9 [44800/240000 (19%)]\tLoss: 90.978928, Average: 1.421546\n",
      "Train Epoch: 9 [51200/240000 (21%)]\tLoss: 103.923264, Average: 1.623801\n",
      "Train Epoch: 9 [57600/240000 (24%)]\tLoss: 81.660614, Average: 1.275947\n",
      "Train Epoch: 9 [64000/240000 (27%)]\tLoss: 81.606339, Average: 1.275099\n",
      "Train Epoch: 9 [70400/240000 (29%)]\tLoss: 97.881569, Average: 1.529400\n",
      "Train Epoch: 9 [76800/240000 (32%)]\tLoss: 90.937775, Average: 1.420903\n",
      "Train Epoch: 9 [83200/240000 (35%)]\tLoss: 86.851463, Average: 1.357054\n",
      "Train Epoch: 9 [89600/240000 (37%)]\tLoss: 88.679367, Average: 1.385615\n",
      "Train Epoch: 9 [96000/240000 (40%)]\tLoss: 84.136078, Average: 1.314626\n",
      "Train Epoch: 9 [102400/240000 (43%)]\tLoss: 76.755318, Average: 1.199302\n",
      "Train Epoch: 9 [108800/240000 (45%)]\tLoss: 89.298874, Average: 1.395295\n",
      "Train Epoch: 9 [115200/240000 (48%)]\tLoss: 116.677948, Average: 1.823093\n",
      "Train Epoch: 9 [121600/240000 (51%)]\tLoss: 64.702469, Average: 1.010976\n",
      "Train Epoch: 9 [128000/240000 (53%)]\tLoss: 90.932671, Average: 1.420823\n",
      "Train Epoch: 9 [134400/240000 (56%)]\tLoss: 71.128006, Average: 1.111375\n",
      "Train Epoch: 9 [140800/240000 (59%)]\tLoss: 88.293282, Average: 1.379583\n",
      "Train Epoch: 9 [147200/240000 (61%)]\tLoss: 85.510391, Average: 1.336100\n",
      "Train Epoch: 9 [153600/240000 (64%)]\tLoss: 75.585899, Average: 1.181030\n",
      "Train Epoch: 9 [160000/240000 (67%)]\tLoss: 97.723969, Average: 1.526937\n",
      "Train Epoch: 9 [166400/240000 (69%)]\tLoss: 93.334320, Average: 1.458349\n",
      "Train Epoch: 9 [172800/240000 (72%)]\tLoss: 79.697189, Average: 1.245269\n",
      "Train Epoch: 9 [179200/240000 (75%)]\tLoss: 77.264809, Average: 1.207263\n",
      "Train Epoch: 9 [185600/240000 (77%)]\tLoss: 98.621315, Average: 1.540958\n",
      "Train Epoch: 9 [192000/240000 (80%)]\tLoss: 90.900337, Average: 1.420318\n",
      "Train Epoch: 9 [198400/240000 (83%)]\tLoss: 91.633980, Average: 1.431781\n",
      "Train Epoch: 9 [204800/240000 (85%)]\tLoss: 77.643517, Average: 1.213180\n",
      "Train Epoch: 9 [211200/240000 (88%)]\tLoss: 94.871246, Average: 1.482363\n",
      "Train Epoch: 9 [217600/240000 (91%)]\tLoss: 85.563553, Average: 1.336931\n",
      "Train Epoch: 9 [224000/240000 (93%)]\tLoss: 96.582741, Average: 1.509105\n",
      "Train Epoch: 9 [230400/240000 (96%)]\tLoss: 91.703751, Average: 1.432871\n",
      "Train Epoch: 9 [236800/240000 (99%)]\tLoss: 80.838966, Average: 1.263109\n",
      "\n",
      "Train set: Average loss: 1.3739\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6233, Accuracy: 38080/40000 (95%)\n",
      "\n",
      "Train Epoch: 10 [0/240000 (0%)]\tLoss: 92.844833, Average: 1.450701\n",
      "Train Epoch: 10 [6400/240000 (3%)]\tLoss: 108.135315, Average: 1.689614\n",
      "Train Epoch: 10 [12800/240000 (5%)]\tLoss: 82.783913, Average: 1.293499\n",
      "Train Epoch: 10 [19200/240000 (8%)]\tLoss: 74.874626, Average: 1.169916\n",
      "Train Epoch: 10 [25600/240000 (11%)]\tLoss: 81.765373, Average: 1.277584\n",
      "Train Epoch: 10 [32000/240000 (13%)]\tLoss: 86.027657, Average: 1.344182\n",
      "Train Epoch: 10 [38400/240000 (16%)]\tLoss: 95.023209, Average: 1.484738\n",
      "Train Epoch: 10 [44800/240000 (19%)]\tLoss: 92.874481, Average: 1.451164\n",
      "Train Epoch: 10 [51200/240000 (21%)]\tLoss: 108.178268, Average: 1.690285\n",
      "Train Epoch: 10 [57600/240000 (24%)]\tLoss: 120.493790, Average: 1.882715\n",
      "Train Epoch: 10 [64000/240000 (27%)]\tLoss: 99.599365, Average: 1.556240\n",
      "Train Epoch: 10 [70400/240000 (29%)]\tLoss: 80.094688, Average: 1.251480\n",
      "Train Epoch: 10 [76800/240000 (32%)]\tLoss: 63.995750, Average: 0.999934\n",
      "Train Epoch: 10 [83200/240000 (35%)]\tLoss: 80.720253, Average: 1.261254\n",
      "Train Epoch: 10 [89600/240000 (37%)]\tLoss: 61.880554, Average: 0.966884\n",
      "Train Epoch: 10 [96000/240000 (40%)]\tLoss: 93.446182, Average: 1.460097\n",
      "Train Epoch: 10 [102400/240000 (43%)]\tLoss: 75.218071, Average: 1.175282\n",
      "Train Epoch: 10 [108800/240000 (45%)]\tLoss: 85.966263, Average: 1.343223\n",
      "Train Epoch: 10 [115200/240000 (48%)]\tLoss: 88.968849, Average: 1.390138\n",
      "Train Epoch: 10 [121600/240000 (51%)]\tLoss: 75.310089, Average: 1.176720\n",
      "Train Epoch: 10 [128000/240000 (53%)]\tLoss: 79.128113, Average: 1.236377\n",
      "Train Epoch: 10 [134400/240000 (56%)]\tLoss: 91.602989, Average: 1.431297\n",
      "Train Epoch: 10 [140800/240000 (59%)]\tLoss: 92.172668, Average: 1.440198\n",
      "Train Epoch: 10 [147200/240000 (61%)]\tLoss: 78.800911, Average: 1.231264\n",
      "Train Epoch: 10 [153600/240000 (64%)]\tLoss: 100.240768, Average: 1.566262\n",
      "Train Epoch: 10 [160000/240000 (67%)]\tLoss: 109.366554, Average: 1.708852\n",
      "Train Epoch: 10 [166400/240000 (69%)]\tLoss: 91.765556, Average: 1.433837\n",
      "Train Epoch: 10 [172800/240000 (72%)]\tLoss: 85.515419, Average: 1.336178\n",
      "Train Epoch: 10 [179200/240000 (75%)]\tLoss: 100.526230, Average: 1.570722\n",
      "Train Epoch: 10 [185600/240000 (77%)]\tLoss: 72.954491, Average: 1.139914\n",
      "Train Epoch: 10 [192000/240000 (80%)]\tLoss: 85.050224, Average: 1.328910\n",
      "Train Epoch: 10 [198400/240000 (83%)]\tLoss: 89.809105, Average: 1.403267\n",
      "Train Epoch: 10 [204800/240000 (85%)]\tLoss: 94.039352, Average: 1.469365\n",
      "Train Epoch: 10 [211200/240000 (88%)]\tLoss: 90.012268, Average: 1.406442\n",
      "Train Epoch: 10 [217600/240000 (91%)]\tLoss: 86.818268, Average: 1.356535\n",
      "Train Epoch: 10 [224000/240000 (93%)]\tLoss: 91.494606, Average: 1.429603\n",
      "Train Epoch: 10 [230400/240000 (96%)]\tLoss: 75.585327, Average: 1.181021\n",
      "Train Epoch: 10 [236800/240000 (99%)]\tLoss: 87.490486, Average: 1.367039\n",
      "\n",
      "Train set: Average loss: 1.3697\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6198, Accuracy: 38183/40000 (95%)\n",
      "\n",
      "Train Epoch: 11 [0/240000 (0%)]\tLoss: 97.226608, Average: 1.519166\n",
      "Train Epoch: 11 [6400/240000 (3%)]\tLoss: 105.450996, Average: 1.647672\n",
      "Train Epoch: 11 [12800/240000 (5%)]\tLoss: 87.281052, Average: 1.363766\n",
      "Train Epoch: 11 [19200/240000 (8%)]\tLoss: 80.534538, Average: 1.258352\n",
      "Train Epoch: 11 [25600/240000 (11%)]\tLoss: 83.397346, Average: 1.303084\n",
      "Train Epoch: 11 [32000/240000 (13%)]\tLoss: 94.514633, Average: 1.476791\n",
      "Train Epoch: 11 [38400/240000 (16%)]\tLoss: 92.627411, Average: 1.447303\n",
      "Train Epoch: 11 [44800/240000 (19%)]\tLoss: 100.381966, Average: 1.568468\n",
      "Train Epoch: 11 [51200/240000 (21%)]\tLoss: 103.594582, Average: 1.618665\n",
      "Train Epoch: 11 [57600/240000 (24%)]\tLoss: 83.286545, Average: 1.301352\n",
      "Train Epoch: 11 [64000/240000 (27%)]\tLoss: 83.561478, Average: 1.305648\n",
      "Train Epoch: 11 [70400/240000 (29%)]\tLoss: 91.005775, Average: 1.421965\n",
      "Train Epoch: 11 [76800/240000 (32%)]\tLoss: 81.838020, Average: 1.278719\n",
      "Train Epoch: 11 [83200/240000 (35%)]\tLoss: 82.811394, Average: 1.293928\n",
      "Train Epoch: 11 [89600/240000 (37%)]\tLoss: 73.726891, Average: 1.151983\n",
      "Train Epoch: 11 [96000/240000 (40%)]\tLoss: 88.491974, Average: 1.382687\n",
      "Train Epoch: 11 [102400/240000 (43%)]\tLoss: 76.915512, Average: 1.201805\n",
      "Train Epoch: 11 [108800/240000 (45%)]\tLoss: 100.040237, Average: 1.563129\n",
      "Train Epoch: 11 [115200/240000 (48%)]\tLoss: 100.023811, Average: 1.562872\n",
      "Train Epoch: 11 [121600/240000 (51%)]\tLoss: 91.510971, Average: 1.429859\n",
      "Train Epoch: 11 [128000/240000 (53%)]\tLoss: 86.921967, Average: 1.358156\n",
      "Train Epoch: 11 [134400/240000 (56%)]\tLoss: 80.144150, Average: 1.252252\n",
      "Train Epoch: 11 [140800/240000 (59%)]\tLoss: 82.997726, Average: 1.296839\n",
      "Train Epoch: 11 [147200/240000 (61%)]\tLoss: 89.804184, Average: 1.403190\n",
      "Train Epoch: 11 [153600/240000 (64%)]\tLoss: 97.399040, Average: 1.521860\n",
      "Train Epoch: 11 [160000/240000 (67%)]\tLoss: 102.915298, Average: 1.608052\n",
      "Train Epoch: 11 [166400/240000 (69%)]\tLoss: 78.558075, Average: 1.227470\n",
      "Train Epoch: 11 [172800/240000 (72%)]\tLoss: 77.271988, Average: 1.207375\n",
      "Train Epoch: 11 [179200/240000 (75%)]\tLoss: 90.242920, Average: 1.410046\n",
      "Train Epoch: 11 [185600/240000 (77%)]\tLoss: 89.132462, Average: 1.392695\n",
      "Train Epoch: 11 [192000/240000 (80%)]\tLoss: 86.477875, Average: 1.351217\n",
      "Train Epoch: 11 [198400/240000 (83%)]\tLoss: 95.256866, Average: 1.488389\n",
      "Train Epoch: 11 [204800/240000 (85%)]\tLoss: 75.073555, Average: 1.173024\n",
      "Train Epoch: 11 [211200/240000 (88%)]\tLoss: 87.554207, Average: 1.368034\n",
      "Train Epoch: 11 [217600/240000 (91%)]\tLoss: 71.920341, Average: 1.123755\n",
      "Train Epoch: 11 [224000/240000 (93%)]\tLoss: 75.650230, Average: 1.182035\n",
      "Train Epoch: 11 [230400/240000 (96%)]\tLoss: 83.694702, Average: 1.307730\n",
      "Train Epoch: 11 [236800/240000 (99%)]\tLoss: 81.286972, Average: 1.270109\n",
      "\n",
      "Train set: Average loss: 1.3649\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.6099, Accuracy: 38293/40000 (96%)\n",
      "\n",
      "Train Epoch: 12 [0/240000 (0%)]\tLoss: 75.466103, Average: 1.179158\n",
      "Train Epoch: 12 [6400/240000 (3%)]\tLoss: 100.738647, Average: 1.574041\n",
      "Train Epoch: 12 [12800/240000 (5%)]\tLoss: 94.099281, Average: 1.470301\n",
      "Train Epoch: 12 [19200/240000 (8%)]\tLoss: 90.013405, Average: 1.406459\n",
      "Train Epoch: 12 [25600/240000 (11%)]\tLoss: 95.140282, Average: 1.486567\n",
      "Train Epoch: 12 [32000/240000 (13%)]\tLoss: 105.107635, Average: 1.642307\n",
      "Train Epoch: 12 [38400/240000 (16%)]\tLoss: 101.899277, Average: 1.592176\n",
      "Train Epoch: 12 [44800/240000 (19%)]\tLoss: 97.940247, Average: 1.530316\n",
      "Train Epoch: 12 [51200/240000 (21%)]\tLoss: 104.915642, Average: 1.639307\n",
      "Train Epoch: 12 [57600/240000 (24%)]\tLoss: 97.511520, Average: 1.523618\n",
      "Train Epoch: 12 [64000/240000 (27%)]\tLoss: 87.652687, Average: 1.369573\n",
      "Train Epoch: 12 [70400/240000 (29%)]\tLoss: 74.220299, Average: 1.159692\n",
      "Train Epoch: 12 [76800/240000 (32%)]\tLoss: 90.497711, Average: 1.414027\n",
      "Train Epoch: 12 [83200/240000 (35%)]\tLoss: 87.268250, Average: 1.363566\n",
      "Train Epoch: 12 [89600/240000 (37%)]\tLoss: 98.844513, Average: 1.544446\n",
      "Train Epoch: 12 [96000/240000 (40%)]\tLoss: 90.045120, Average: 1.406955\n",
      "Train Epoch: 12 [102400/240000 (43%)]\tLoss: 70.280777, Average: 1.098137\n",
      "Train Epoch: 12 [108800/240000 (45%)]\tLoss: 86.079880, Average: 1.344998\n",
      "Train Epoch: 12 [115200/240000 (48%)]\tLoss: 104.044098, Average: 1.625689\n",
      "Train Epoch: 12 [121600/240000 (51%)]\tLoss: 84.230133, Average: 1.316096\n",
      "Train Epoch: 12 [128000/240000 (53%)]\tLoss: 84.220535, Average: 1.315946\n",
      "Train Epoch: 12 [134400/240000 (56%)]\tLoss: 64.070381, Average: 1.001100\n",
      "Train Epoch: 12 [140800/240000 (59%)]\tLoss: 75.654724, Average: 1.182105\n",
      "Train Epoch: 12 [147200/240000 (61%)]\tLoss: 77.521881, Average: 1.211279\n",
      "Train Epoch: 12 [153600/240000 (64%)]\tLoss: 80.843643, Average: 1.263182\n",
      "Train Epoch: 12 [160000/240000 (67%)]\tLoss: 104.568756, Average: 1.633887\n",
      "Train Epoch: 12 [166400/240000 (69%)]\tLoss: 80.068176, Average: 1.251065\n",
      "Train Epoch: 12 [172800/240000 (72%)]\tLoss: 78.147133, Average: 1.221049\n",
      "Train Epoch: 12 [179200/240000 (75%)]\tLoss: 90.046616, Average: 1.406978\n",
      "Train Epoch: 12 [185600/240000 (77%)]\tLoss: 101.874565, Average: 1.591790\n",
      "Train Epoch: 12 [192000/240000 (80%)]\tLoss: 79.972046, Average: 1.249563\n",
      "Train Epoch: 12 [198400/240000 (83%)]\tLoss: 90.990623, Average: 1.421728\n",
      "Train Epoch: 12 [204800/240000 (85%)]\tLoss: 80.987373, Average: 1.265428\n",
      "Train Epoch: 12 [211200/240000 (88%)]\tLoss: 95.938805, Average: 1.499044\n",
      "Train Epoch: 12 [217600/240000 (91%)]\tLoss: 95.129005, Average: 1.486391\n",
      "Train Epoch: 12 [224000/240000 (93%)]\tLoss: 96.359375, Average: 1.505615\n",
      "Train Epoch: 12 [230400/240000 (96%)]\tLoss: 89.128807, Average: 1.392638\n",
      "Train Epoch: 12 [236800/240000 (99%)]\tLoss: 98.842438, Average: 1.544413\n",
      "\n",
      "Train set: Average loss: 1.3631\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.5893, Accuracy: 38346/40000 (96%)\n",
      "\n",
      "Train Epoch: 13 [0/240000 (0%)]\tLoss: 80.026955, Average: 1.250421\n",
      "Train Epoch: 13 [6400/240000 (3%)]\tLoss: 72.906082, Average: 1.139158\n",
      "Train Epoch: 13 [12800/240000 (5%)]\tLoss: 94.766281, Average: 1.480723\n",
      "Train Epoch: 13 [19200/240000 (8%)]\tLoss: 97.764343, Average: 1.527568\n",
      "Train Epoch: 13 [25600/240000 (11%)]\tLoss: 82.073868, Average: 1.282404\n",
      "Train Epoch: 13 [32000/240000 (13%)]\tLoss: 86.669205, Average: 1.354206\n",
      "Train Epoch: 13 [38400/240000 (16%)]\tLoss: 80.597244, Average: 1.259332\n",
      "Train Epoch: 13 [44800/240000 (19%)]\tLoss: 89.555374, Average: 1.399303\n",
      "Train Epoch: 13 [51200/240000 (21%)]\tLoss: 99.992538, Average: 1.562383\n",
      "Train Epoch: 13 [57600/240000 (24%)]\tLoss: 97.892723, Average: 1.529574\n",
      "Train Epoch: 13 [64000/240000 (27%)]\tLoss: 71.143875, Average: 1.111623\n",
      "Train Epoch: 13 [70400/240000 (29%)]\tLoss: 82.039070, Average: 1.281860\n",
      "Train Epoch: 13 [76800/240000 (32%)]\tLoss: 87.371307, Average: 1.365177\n",
      "Train Epoch: 13 [83200/240000 (35%)]\tLoss: 87.993393, Average: 1.374897\n",
      "Train Epoch: 13 [89600/240000 (37%)]\tLoss: 82.582596, Average: 1.290353\n",
      "Train Epoch: 13 [96000/240000 (40%)]\tLoss: 82.867065, Average: 1.294798\n",
      "Train Epoch: 13 [102400/240000 (43%)]\tLoss: 73.393639, Average: 1.146776\n",
      "Train Epoch: 13 [108800/240000 (45%)]\tLoss: 95.100548, Average: 1.485946\n",
      "Train Epoch: 13 [115200/240000 (48%)]\tLoss: 92.828979, Average: 1.450453\n",
      "Train Epoch: 13 [121600/240000 (51%)]\tLoss: 83.076950, Average: 1.298077\n",
      "Train Epoch: 13 [128000/240000 (53%)]\tLoss: 84.013420, Average: 1.312710\n",
      "Train Epoch: 13 [134400/240000 (56%)]\tLoss: 97.923882, Average: 1.530061\n",
      "Train Epoch: 13 [140800/240000 (59%)]\tLoss: 80.259399, Average: 1.254053\n",
      "Train Epoch: 13 [147200/240000 (61%)]\tLoss: 83.350639, Average: 1.302354\n",
      "Train Epoch: 13 [153600/240000 (64%)]\tLoss: 89.431984, Average: 1.397375\n",
      "Train Epoch: 13 [160000/240000 (67%)]\tLoss: 83.111229, Average: 1.298613\n",
      "Train Epoch: 13 [166400/240000 (69%)]\tLoss: 90.833496, Average: 1.419273\n",
      "Train Epoch: 13 [172800/240000 (72%)]\tLoss: 93.310074, Average: 1.457970\n",
      "Train Epoch: 13 [179200/240000 (75%)]\tLoss: 92.104530, Average: 1.439133\n",
      "Train Epoch: 13 [185600/240000 (77%)]\tLoss: 75.783562, Average: 1.184118\n",
      "Train Epoch: 13 [192000/240000 (80%)]\tLoss: 81.167931, Average: 1.268249\n",
      "Train Epoch: 13 [198400/240000 (83%)]\tLoss: 77.998550, Average: 1.218727\n",
      "Train Epoch: 13 [204800/240000 (85%)]\tLoss: 65.948433, Average: 1.030444\n",
      "Train Epoch: 13 [211200/240000 (88%)]\tLoss: 103.259308, Average: 1.613427\n",
      "Train Epoch: 13 [217600/240000 (91%)]\tLoss: 76.947769, Average: 1.202309\n",
      "Train Epoch: 13 [224000/240000 (93%)]\tLoss: 60.210514, Average: 0.940789\n",
      "Train Epoch: 13 [230400/240000 (96%)]\tLoss: 91.390541, Average: 1.427977\n",
      "Train Epoch: 13 [236800/240000 (99%)]\tLoss: 81.944527, Average: 1.280383\n",
      "\n",
      "Train set: Average loss: 1.3560\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.5926, Accuracy: 38441/40000 (96%)\n",
      "\n",
      "Train Epoch: 14 [0/240000 (0%)]\tLoss: 84.788727, Average: 1.324824\n",
      "Train Epoch: 14 [6400/240000 (3%)]\tLoss: 95.490799, Average: 1.492044\n",
      "Train Epoch: 14 [12800/240000 (5%)]\tLoss: 73.525261, Average: 1.148832\n",
      "Train Epoch: 14 [19200/240000 (8%)]\tLoss: 84.935486, Average: 1.327117\n",
      "Train Epoch: 14 [25600/240000 (11%)]\tLoss: 90.857880, Average: 1.419654\n",
      "Train Epoch: 14 [32000/240000 (13%)]\tLoss: 100.351173, Average: 1.567987\n",
      "Train Epoch: 14 [38400/240000 (16%)]\tLoss: 104.143250, Average: 1.627238\n",
      "Train Epoch: 14 [44800/240000 (19%)]\tLoss: 82.051414, Average: 1.282053\n",
      "Train Epoch: 14 [51200/240000 (21%)]\tLoss: 99.583725, Average: 1.555996\n",
      "Train Epoch: 14 [57600/240000 (24%)]\tLoss: 85.080994, Average: 1.329391\n",
      "Train Epoch: 14 [64000/240000 (27%)]\tLoss: 87.919411, Average: 1.373741\n",
      "Train Epoch: 14 [70400/240000 (29%)]\tLoss: 91.876251, Average: 1.435566\n",
      "Train Epoch: 14 [76800/240000 (32%)]\tLoss: 82.555374, Average: 1.289928\n",
      "Train Epoch: 14 [83200/240000 (35%)]\tLoss: 84.831352, Average: 1.325490\n",
      "Train Epoch: 14 [89600/240000 (37%)]\tLoss: 75.727852, Average: 1.183248\n",
      "Train Epoch: 14 [96000/240000 (40%)]\tLoss: 75.954643, Average: 1.186791\n",
      "Train Epoch: 14 [102400/240000 (43%)]\tLoss: 66.763718, Average: 1.043183\n",
      "Train Epoch: 14 [108800/240000 (45%)]\tLoss: 81.440552, Average: 1.272509\n",
      "Train Epoch: 14 [115200/240000 (48%)]\tLoss: 101.422745, Average: 1.584730\n",
      "Train Epoch: 14 [121600/240000 (51%)]\tLoss: 86.448120, Average: 1.350752\n",
      "Train Epoch: 14 [128000/240000 (53%)]\tLoss: 93.426155, Average: 1.459784\n",
      "Train Epoch: 14 [134400/240000 (56%)]\tLoss: 74.207977, Average: 1.159500\n",
      "Train Epoch: 14 [140800/240000 (59%)]\tLoss: 68.597176, Average: 1.071831\n",
      "Train Epoch: 14 [147200/240000 (61%)]\tLoss: 79.129738, Average: 1.236402\n",
      "Train Epoch: 14 [153600/240000 (64%)]\tLoss: 91.240845, Average: 1.425638\n",
      "Train Epoch: 14 [160000/240000 (67%)]\tLoss: 103.917122, Average: 1.623705\n",
      "Train Epoch: 14 [166400/240000 (69%)]\tLoss: 91.404533, Average: 1.428196\n",
      "Train Epoch: 14 [172800/240000 (72%)]\tLoss: 113.660507, Average: 1.775945\n",
      "Train Epoch: 14 [179200/240000 (75%)]\tLoss: 94.276337, Average: 1.473068\n",
      "Train Epoch: 14 [185600/240000 (77%)]\tLoss: 90.928612, Average: 1.420760\n",
      "Train Epoch: 14 [192000/240000 (80%)]\tLoss: 90.360527, Average: 1.411883\n",
      "Train Epoch: 14 [198400/240000 (83%)]\tLoss: 86.664619, Average: 1.354135\n",
      "Train Epoch: 14 [204800/240000 (85%)]\tLoss: 101.122147, Average: 1.580034\n",
      "Train Epoch: 14 [211200/240000 (88%)]\tLoss: 73.543297, Average: 1.149114\n",
      "Train Epoch: 14 [217600/240000 (91%)]\tLoss: 80.332344, Average: 1.255193\n",
      "Train Epoch: 14 [224000/240000 (93%)]\tLoss: 84.082397, Average: 1.313787\n",
      "Train Epoch: 14 [230400/240000 (96%)]\tLoss: 100.797531, Average: 1.574961\n",
      "Train Epoch: 14 [236800/240000 (99%)]\tLoss: 84.257187, Average: 1.316519\n",
      "\n",
      "Train set: Average loss: 1.3506\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.5636, Accuracy: 38468/40000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "use_cuda = False\n",
    "torch.manual_seed(33)\n",
    "\n",
    "if use_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "epochs = 15\n",
    "lr = 0.00001\n",
    "\n",
    "model = Net().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Guardam el valor de peèrdua mig de cada iteració (època)\n",
    "train_l = np.zeros((epochs))\n",
    "test_l = np.zeros((epochs))\n",
    "\n",
    "# Bucle d'entrenament\n",
    "for epoch in range(0, epochs):\n",
    "    train_l[epoch] = train(model, device, train_loader, optimizer, epoch)\n",
    "    test_l[epoch] = test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wjeMWK8cJkqN"
   },
   "source": [
    "## Resultats\n",
    "\n",
    "Aquí visualitzarem els resultats d'aprenentatge de la xarxa.\n",
    "\n",
    "### Feina a fer:\n",
    "\n",
    "1. Fer una predicció del primer _batch_ del conjunt de _test_.\n",
    "2. Visualitzar una imatge del _batch_ i posar la predicció i el groun truth com a títol de la imatge.\n",
    "3. Visualitzar el resultat de la mateixa imatge passada per tots els filtres de la primera convolució de la vostra xarxa.\n",
    "4. **Extra**: Fer la matriu de confusió de les 10 classes per poder entendre el que no estau fent bé (la xarxa no està fent bé).\n",
    "\n",
    "A tenir en compte:\n",
    "\n",
    "#### Subplots\n",
    "\n",
    "Per fer graelles d'imatges podeu empar la funció `subplots`. Més [informació](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.subplots.html)\n",
    "\n",
    "#### Device\n",
    "\n",
    "Si heu emprat _GPU_ per accelerar el procés d'entrenament, els resultats que obtenim de la xarxa també seràn a la _GPU_. **Pytorch** proporciona la funció `cpu()` que retorna una còpia d'aquest objecte a la memòria de la CPU.\n",
    "\n",
    "#### Detach\n",
    "\n",
    "Per poder operar amb els resultats de la predicció emprarem la funció `detach` que retorna un nou Tensor \"separat\" del graf (xarxa) en curs.\n",
    "\n",
    "Per tant per transformar el tensor que retorna la xarxa en un array de la lliberia _Numpy_ caldria fer el següent:\n",
    "\n",
    "```\n",
    "resultat_np = resultat.detach().numpy()\n",
    "```\n",
    "\n",
    "Si a més hem executat l'entrenament en _GPU_:\n",
    "\n",
    "```\n",
    "resultat_np = resultat.cpu().detach().numpy()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SYKUppOc_4JE"
   },
   "outputs": [],
   "source": [
    "def show_images_with_pred(images, labels, predictions):\n",
    "    model.eval()\n",
    "\n",
    "    n_images = len(images)\n",
    "    fig, axs = plt.subplots(\n",
    "        int(n_images**0.5),\n",
    "        int(n_images**0.5),\n",
    "        figsize=(n_images // 4, n_images // 4),\n",
    "    )\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    for image, label, predict, ax in zip(images, labels, predictions, axs):\n",
    "        ax.imshow(image.reshape(*image.shape[-2::]))\n",
    "        ax.set_title(f\"Label: {label}, Prediction: {predict}\")\n",
    "        ax.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "data, labels = next(iter(test_loader))\n",
    "model.eval()\n",
    "predictions, layers = model(data)\n",
    "predictions = np.argmax(predictions.detach().cpu().numpy(), axis=1)\n",
    "\n",
    "show_images_with_pred(data, labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2400x1200 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1200 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# select convolution layers\n",
    "conv_layers = [x[1] for x in filter(lambda x: x[0] == \"conv\", layers)]\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 4, figsize=(24,12))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for idx, ax in enumerate(axs):\n",
    "    ax.imshow(conv_layers[0].detach().cpu().numpy()[1, idx, :, :])\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(4, 4, figsize=(12,12))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for idx, ax in enumerate(axs):\n",
    "    ax.imshow(conv_layers[1].detach().cpu().numpy()[1, idx, :, :])\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    truth = []\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            predictions.extend(target.tolist())\n",
    "            output, _ = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max probability\n",
    "            truth.extend(torch.flatten(pred.cpu()).tolist())\n",
    "\n",
    "    return truth, predictions\n",
    "\n",
    "cm = confusion_matrix(*predict(model, next(iter(test_loader))))\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d')\n",
    "plt.xlabel('Prediccions')\n",
    "plt.ylabel('Ground Truth')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
